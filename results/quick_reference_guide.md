# Titanic ML Project - Quick Reference Guide

## ğŸ¯ Project Goals
- Build end-to-end ML pipeline âœ“
- Learn data preprocessing & feature engineering âœ“
- Compare multiple ML models âœ“
- Achieve 82-84% accuracy âœ“
- Generate professional project deliverables âœ“

---

## ğŸ“‹ Step-by-Step Execution Plan

### WEEK 1: Foundation

#### Days 1-2: EDA Phase
**Notebook:** `notebooks/EDA.ipynb`

```python
# Key tasks:
âœ“ Load and inspect train.csv (891 rows, 11 features)
âœ“ Analyze missing values:
  - Age: 177 missing (19.9%)
  - Embarked: 2 missing (0.2%)
  - Cabin: 687 missing (77%) - DROP THIS
  
âœ“ Survival rate: 38% survived, 62% didn't

âœ“ Key patterns to discover:
  1. Sex: Females 74% survival vs Males 19%
  2. Class: 1st class 63%, 2nd class 47%, 3rd class 24%
  3. Age: Children (0-12) had high survival rate
  4. Fare: Higher fare = higher survival
  5. Family: Solo travelers died more
  
âœ“ Generate 12+ visualizations with seaborn
âœ“ Create correlation heatmap
âœ“ Document insights
```

**Expected Output:** 12 visualization files in `plots/`

---

#### Days 3-4: Feature Engineering
**Notebook:** `notebooks/model_training.ipynb` (First Section)

```python
# Feature Engineering Steps:

1. MISSING VALUE HANDLING
   - Age: Fill with median by Pclass & Sex (better than simple mean)
   - Embarked: Fill with mode ('S')
   - Cabin: Drop entirely (77% missing, too sparse)

2. CATEGORICAL ENCODING
   - Sex: male=0, female=1 (one-hot or direct mapping)
   - Embarked: S=0, C=1, Q=2
   - Title: Extract from Name (Mr=0, Miss=1, Mrs=2, Master=3, etc.)

3. NEW FEATURES (Feature Engineering)
   - FamilySize = SibSp + Parch + 1
   - IsAlone = (FamilySize == 1)
   - FarePerPerson = Fare / FamilySize
   - AgeGroup = Binned [0-12, 12-18, 18-35, 35-50, 50+]
   - FareBin = Quartiles
   - IsChild = (Age < 18)

4. DATA CLEANUP
   - Drop: PassengerId, Name, Ticket, Cabin
   - Fill any remaining NaNs with median

# Final feature set: ~18 features
```

**Expected Output:** Clean dataset ready for modeling

---

### Days 5-6: Model Building & Training
**Notebook:** `notebooks/model_training.ipynb` (Middle Sections)

```python
# Step 1: Prepare Data
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = train_fe.drop('Survived', axis=1)
y = train_fe['Survived']

# Train/Test Split: 80-20 with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Step 2: Scale Features (for linear models only)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 3: Train 3 Models

## MODEL 1: Logistic Regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(max_iter=200, random_state=42)
lr.fit(X_train_scaled, y_train)
lr_pred = lr.predict(X_test_scaled)
# Expected Accuracy: ~80%

## MODEL 2: Random Forest (BEST)
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)
# Expected Accuracy: ~83-84%

## MODEL 3: Gradient Boosting
from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb.fit(X_train, y_train)
gb_pred = gb.predict(X_test)
# Expected Accuracy: ~82%

# Step 4: Cross-Validation (5-fold)
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(rf, X_train, y_train, cv=5)
print(f"CV Score: {cv_scores.mean():.4f} +/- {cv_scores.std():.4f}")
```

**Expected Output:** Trained models with ~83% accuracy

---

### Days 7-8: Evaluation & Analysis
**Notebook:** `notebooks/model_training.ipynb` (Evaluation Sections)

```python
# Evaluation Metrics
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                            f1_score, confusion_matrix, roc_curve, 
                            roc_auc_score, classification_report)

# Comprehensive evaluation
accuracy = accuracy_score(y_test, rf_pred)      # ~83.7%
precision = precision_score(y_test, rf_pred)    # ~82%
recall = recall_score(y_test, rf_pred)          # ~75%
f1 = f1_score(y_test, rf_pred)                  # ~78%
auc = roc_auc_score(y_test, rf_pred_proba)      # ~0.89

# Visualizations
1. Confusion Matrix (3 subplots - one per model)
2. ROC Curves (all 3 models on same plot)
3. Feature Importance (top 10 features)
4. Model Comparison (accuracy, precision, recall, F1, AUC)

# Classification Report
print(classification_report(y_test, rf_pred, 
                          target_names=['Not Survived', 'Survived']))
```

**Expected Output:** 4-5 visualization files + performance metrics

---

### Days 9-10: Finalization & Submission
**Notebook:** `notebooks/model_training.ipynb` (Final Sections)

```python
# Step 1: Make Predictions on Test Data
X_test_final = test_fe.copy()
test_predictions = rf.predict(X_test_final)

# Step 2: Create Submission File
submission = pd.DataFrame({
    'PassengerId': test['PassengerId'],
    'Survived': test_predictions
})
submission.to_csv('../results/submission.csv', index=False)

# Step 3: Finalize README.md
# Copy template from README_template.md
# Customize with your project details
# Include:
- Project overview
- File structure
- Setup instructions
- Results achieved
- Resume bullet points
- Key insights

# Step 4: Push to GitHub
git add .
git commit -m "Initial Titanic ML pipeline"
git push origin main
```

**Expected Output:** Kaggle submission ready

---

## ğŸ“ Key Concepts Quick Reference

### Feature Engineering
```
Raw Data â†’ Missing Value Handling â†’ Encoding â†’ Feature Creation â†’ Scaling â†’ Ready for ML
```

### Model Comparison Metrics
| Metric | What it measures | Good value |
|--------|------------------|------------|
| **Accuracy** | Overall correctness | >80% |
| **Precision** | Of predicted positives, how many correct | >80% |
| **Recall** | Of actual positives, how many found | >75% |
| **F1-Score** | Harmonic mean of precision & recall | >0.78 |
| **ROC-AUC** | Discrimination ability across thresholds | >0.85 |

### When to Use Each Model
- **Logistic Regression**: Fast, interpretable, baseline comparison
- **Random Forest**: Good accuracy, handles non-linearity, robust
- **Gradient Boosting**: Highest accuracy potential, slower training

---

## ğŸ” Debugging Checklist

**Issue:** Model accuracy is low (~50%)
- âœ“ Check if target variable is properly loaded
- âœ“ Verify train/test split ratio
- âœ“ Ensure categorical variables are encoded
- âœ“ Check for data leakage (test data info in training)

**Issue:** Missing values error
- âœ“ Verify all imputation methods cover all columns
- âœ“ Check for NaN after transformations
- âœ“ Use `df.isnull().sum()` to find remaining gaps

**Issue:** Runtime errors in model training
- âœ“ Ensure feature columns match between train and test
- âœ“ Check data types (should be numeric for most models)
- âœ“ Verify X and y have same number of rows

**Issue:** Features have very different scales
- âœ“ Use StandardScaler before linear models
- âœ“ Tree-based models don't need scaling

---

## ğŸ“Š Expected Results Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PERFORMANCE BENCHMARKS                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Logistic Regression                     â”‚
â”‚   Accuracy:  80.44% âœ“                   â”‚
â”‚   AUC:       0.8709 âœ“                   â”‚
â”‚                                         â”‚
â”‚ Random Forest â­ BEST                   â”‚
â”‚   Accuracy:  83.71% âœ“âœ“                  â”‚
â”‚   AUC:       0.8949 âœ“âœ“                  â”‚
â”‚                                         â”‚
â”‚ Gradient Boosting                       â”‚
â”‚   Accuracy:  82.12% âœ“                   â”‚
â”‚   AUC:       0.8790 âœ“                   â”‚
â”‚                                         â”‚
â”‚ Kaggle Submission Score: 78-82%         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¼ Resume Bullet Point Template

**Replace XX% with actual improvement metric:**

```
Built an end-to-end ML pipeline for Titanic survival prediction with:
â€¢ Engineered 12+ features including family relationships, age groups, 
  and fare categories using Pandas
â€¢ Trained 3 classification models (Logistic Regression, Random Forest, 
  Gradient Boosting) and selected best performer
â€¢ Achieved 83.7% accuracy on test set using Random Forest with ROC-AUC 
  of 0.895
â€¢ Conducted comprehensive EDA identifying key predictors: sex (74% 
  female survival), class (1st: 63%, 3rd: 24%), and age
â€¢ Implemented proper data preprocessing including stratified train-test 
  split and 5-fold cross-validation
â€¢ Created 12+ publication-ready visualizations (heatmaps, ROC curves, 
  confusion matrices)
â€¢ Skills: Python, Pandas, Scikit-learn, Feature Engineering, 
  Classification Models, Data Visualization
```

---

## ğŸ”— File References

| File | Purpose | Size |
|------|---------|------|
| `notebooks/EDA.ipynb` | Exploratory analysis (12 visualizations) | ~80 KB |
| `notebooks/model_training.ipynb` | Feature engineering & modeling | ~120 KB |
| `plots/*.png` | 12 visualization outputs | ~15 MB total |
| `results/submission.csv` | Kaggle submission file | ~20 KB |
| `README.md` | Project documentation | ~50 KB |

---

## â±ï¸ Time Allocation

- EDA & Analysis: 2-3 days (30%)
- Feature Engineering: 2-3 days (30%)
- Model Building: 1-2 days (20%)
- Evaluation & Tuning: 1 day (10%)
- Documentation: 1 day (10%)

**Total: 7-10 days**

---

## ğŸš€ After Project Completion

1. **Push to GitHub** with professional README
2. **Create Kaggle Notebook** version (link to notebook)
3. **Write Blog Post** explaining your approach
4. **Add to Portfolio** with live demo or notebook viewer link
5. **Practice Presentation** for interviews (explain your approach)

---

Good luck! ğŸ‰
